# =============================================================================
# AI Book Writer - LLM Provider Configuration
# =============================================================================
# Copy this file to .env and configure your preferred LLM provider(s)
# You can use multiple providers simultaneously for different agents

# -----------------------------------------------------------------------------
# Primary LLM Provider Selection
# -----------------------------------------------------------------------------
# Options: "ollama", "anthropic", "gemini", "groq", "openrouter", "deepseek", "together_ai", "openai"
# This sets the default provider for agents unless overridden
DEFAULT_LLM_PROVIDER=groq

# -----------------------------------------------------------------------------
# Ollama Configuration (Local LLMs)
# -----------------------------------------------------------------------------
# Base URL for Ollama server (default: http://localhost:11434)
OLLAMA_BASE_URL=http://localhost:11434

# Default Ollama model to use
# Popular options: llama3.2, deepseek-r1:1.5b, qwen2.5:1.5b, mistral, etc.
OLLAMA_MODEL=llama3.2

# Alternative models for different agents (optional)
OLLAMA_MODEL_STORY_PLANNER=deepseek-r1:1.5b
OLLAMA_MODEL_WRITER=llama3:8b-instruct
OLLAMA_MODEL_EDITOR=qwen2.5:1.5b

# -----------------------------------------------------------------------------
# Anthropic (Claude) Configuration
# -----------------------------------------------------------------------------
# Get your API key from: https://console.anthropic.com/
ANTHROPIC_API_KEY=your_anthropic_api_key_here

# Default Anthropic model
# Options: claude-3-5-sonnet-20241022, claude-3-opus-20240229, claude-3-haiku-20240307
ANTHROPIC_MODEL=claude-3-5-sonnet-20241022

# Max tokens for Claude responses (required by Anthropic API)
ANTHROPIC_MAX_TOKENS=4096

# -----------------------------------------------------------------------------
# Groq Configuration (Fast & Free!)
# -----------------------------------------------------------------------------
# Get your API key from: https://console.groq.com/keys
GROQ_API_KEY=your_groq_api_key_here

# Default Groq model
# Options: llama-3.3-70b-versatile, mixtral-8x7b-32768, llama3-70b-8192
GROQ_MODEL=llama-3.3-70b-versatile

# -----------------------------------------------------------------------------
# Google Gemini Configuration
# -----------------------------------------------------------------------------
# Get your API key from: https://aistudio.google.com/app/apikey
GOOGLE_API_KEY=your_google_api_key_here
# Alternative variable name (both work)
# GEMINI_API_KEY=your_google_api_key_here

# Default Gemini model
# Options: gemini-2.0-flash-exp, gemini-1.5-pro, gemini-1.5-flash
GEMINI_MODEL=gemini-2.0-flash-exp

# -----------------------------------------------------------------------------
# OpenRouter Configuration (Access to 100+ models including Qwen!)
# -----------------------------------------------------------------------------
# Get your API key from: https://openrouter.ai/keys
OPENROUTER_API_KEY=your_openrouter_api_key_here

# Default OpenRouter model
# Free options: qwen/qwen-2.5-72b-instruct, deepseek/deepseek-chat, meta-llama/llama-3.1-70b-instruct
OPENROUTER_MODEL=qwen/qwen-2.5-72b-instruct

# -----------------------------------------------------------------------------
# DeepSeek Configuration (Generous free tier)
# -----------------------------------------------------------------------------
# Get your API key from: https://platform.deepseek.com/
DEEPSEEK_API_KEY=your_deepseek_api_key_here

# Default DeepSeek model
# Options: deepseek-chat, deepseek-coder
DEEPSEEK_MODEL=deepseek-chat

# -----------------------------------------------------------------------------
# Together AI Configuration (Free $25 credits)
# -----------------------------------------------------------------------------
# Get your API key from: https://api.together.xyz/
TOGETHER_API_KEY=your_together_api_key_here

# Default Together AI model
# Options: Qwen/Qwen2.5-72B-Instruct, meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo
TOGETHER_MODEL=Qwen/Qwen2.5-72B-Instruct

# -----------------------------------------------------------------------------
# OpenAI Configuration
# -----------------------------------------------------------------------------
# Get your API key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=your_openai_api_key_here

# Default OpenAI model
# Options: gpt-4o, gpt-4o-mini, gpt-4-turbo
OPENAI_MODEL=gpt-4o-mini

# -----------------------------------------------------------------------------
# LLM Generation Parameters (Applied to all providers)
# -----------------------------------------------------------------------------
# Temperature: Controls randomness (0.0 = deterministic, 1.0 = creative)
LLM_TEMPERATURE=0.7

# Top P: Nucleus sampling threshold (0.0-1.0)
LLM_TOP_P=0.95

# Max tokens: Maximum length of generated responses
LLM_MAX_TOKENS=4096

# -----------------------------------------------------------------------------
# Agent-Specific Overrides (Optional)
# -----------------------------------------------------------------------------
# You can override the provider for specific agents
# Format: AGENT_<AGENT_NAME>_PROVIDER=<provider>
# Example: Use Claude for story planning but Ollama for writing
# AGENT_STORY_PLANNER_PROVIDER=anthropic
# AGENT_WRITER_PROVIDER=ollama
# AGENT_EDITOR_PROVIDER=gemini

# -----------------------------------------------------------------------------
# Advanced Settings
# -----------------------------------------------------------------------------
# Enable verbose logging for LLM calls
LLM_VERBOSE=false

# Enable streaming responses (recommended)
LLM_STREAMING=true

# Timeout for LLM API calls (seconds)
LLM_TIMEOUT=120

# -----------------------------------------------------------------------------
# Project Settings
# -----------------------------------------------------------------------------
# Default genre for new projects
DEFAULT_GENRE=literary_fiction

# Default number of chapters
DEFAULT_NUM_CHAPTERS=12

# Output directory for generated books
OUTPUT_DIR=output
